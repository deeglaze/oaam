\documentclass[preprint,onecolumn,9pt]{sigplanconf} %{onecol}
\usepackage{alltt,mathpartir}
\usepackage{amsmath,amsthm}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{url}
\usepackage{graphicx}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
% \usepackage{xltxtra}
% \setmonofont[Scale=MatchLowercase]{DejaVu Sans Mono}
%
\input{preamble}

\begin{document}

\conferenceinfo{WXYZ '05}{date, City.}
\copyrightyear{2005}
\copyrightdata{[to be supplied]}

% \titlebanner{banner above paper title}        % These are ignored unless
% \preprintfooter{short description of paper}   % 'preprint' option specified.

\title{Optimizing Abstract Abstract Machines}

%% \authorinfo{J. Ian Johnson}
%%            {Northeastern University}
%%            {ianj@ccs.neu.edu}
%% \authorinfo{Matthew Might}
%%            {University of Utah}
%%            {might@cs.utah.edu}
%% \authorinfo{David Van Horn}
%%            {Northeastern University}
%%            {dvanhorn@ccs.neu.edu}
\authorinfo{}{}{}

\maketitle

\begin{abstract}
Abstracting abstract machines has recently been proposed as a
lightweight approach to designing sound and computable program
analyses.  The approach involves deriving abstract interpreters from
existing machine semantics and has been applied to a variety of
languages with features widely considered difficult to analyze.
Although analyzers are straightforward to build under this approach,
the result is prohibitively inefficient.

This article contributes a step by step process for going from a naive
analyzer derived under the abstracting abstract machine approach, to
an efficient program analyzer.
\end{abstract}

%% \category{CR-number}{subcategory}{third-level}

%% \terms
%% term1, term2

%% \keywords
%% keyword1, keyword2

\section{Introduction}

The \emph{abstracting abstract machines} (AAM)
approach~\cite{dvanhorn:VanHorn2011Abstracting,dvanhorn:VanHorn2012Systematic}
to deriving program analyses provides a straightforward way of
transforming a programming language semantics in the form of an
abstract machine, into a family of abstract interpreters parameterized
over policies for regulating the analytic precision of the control,
environment, store, and base value domains.  By taking a
machine-oriented view of computation, it becomes possible to design,
verify, and implement program analyzers for realistic language
features typically considered difficult to model.  The approach was
originally applied to features such as higher-order functions,
stack-inspection, exceptions, laziness, first-class continuations, and
garbage collection.  It has since been used to verify actor-
\cite{local:DOsualdo:12A} and
thread-based~\cite{dvanhorn:Might2011Family} parallelism and
behavioral contracts~\cite{dvanhorn:TobinHochstadt2012Higherorder}; it
has been used to model Coq~\cite{local:harvard},
Dalvik~\cite{local:dalvik}, Erlang~\cite{local:DOsualdo:12B},
JavaScript~\cite{local:DBLP:journals/corr/abs-1109-4467}, and
Racket~\cite{dvanhorn:TobinHochstadt2012Higherorder}.

The primary strength of the approach is that abstract interpreters can
be easily derived through a small number of steps from existing
machine models.  Since the relationships between abstract machines and
higher-level semantic models---such as definitional
interpreters~\cite{dvanhorn:reynolds-hosc98}, structured operational
semantics~\cite{dvanhorn:Plotkin1981Structural}, and reduction
semantics~\cite{dvanhorn:Felleisen2009Semantics}---are well
understood~\cite{dvanhorn:Danvy:DSc}, it is possible to navigate from
these high-level semantic models to sound program analyzers in a
systematic way.  Moreover, since these analyses so closely resemble a
language's interpreter (a) implementing an analysis requires little
more than implementing an interpreter, (b) a single implementation can
serve as both an interpreter and analyzer, and (c) verifying the
correctness of the implementation is straightforward.

However, there is a considerable weakness with the approach: an
analyzer designed and implemented by following the AAM recipe is
prohibitively inefficient without both further approximation and
further implementation effort.

In this article, we develop a systematic approach to deriving the
feasible implementation of an abstract machine based analyzer.




%% Using this approach has also lead to new insights in pushdown
%% abstractions of programs capable of precisely modeling the program
%% stack, even in the presence of control effects.


%% \subsection{Notation and prerequisites}

%% Prereqs: Semantics Engineering \cite{dvanhorn:Felleisen2009Semantics},
%% AAM \cite{dvanhorn:VanHorn2011Abstracting}
%% \cite{dvanhorn:VanHorn2012Systematic}.

%% Notation: concrete examples of programs under analysis is given in
%% (monochrome) Scheme notation.  Code is given in (syntax colored)
%% Racket.

\section{At a Glance}

In the following sections, we develop an analyzer for a core
functional language and follow a series of steps of abstraction and
optimization.  At each step we evaluate the implementation on an
example that computes with Church numerals, shown in
figure~\ref{fig:church}.
%% \footnote{It is
%%   written here using Scheme notation and makes use of the shorthands:
%% %
%% \begin{alltt}
%%    (define (((f x) y) z) ...) \(\equiv\)
%%
%%    (define f (\(\lambda\) (x) (\(\lambda\) (y) (\(\lambda\) (z) ...))))
%% \end{alltt}
%%
%% \begin{alltt}
%%    \church{n} \(\equiv\) (\(\lambda\) (s) (\(\lambda\) (z) (s\(\subscript1\) (s\(\subscript2\) ... (s\(\subscript{n}\) z)))))
%% \end{alltt}}
The example, while small, is informative:
\begin{enumerate}
\item it can be written in most modern programming languages, and
%
\item it stresses an analyzer's ability to deal with complicated
environment and control structure arising from the use of higher-order
functions to model arithmetic.
\end{enumerate}

We start, in section~\ref{sec:aam}, by developing an abstract
interpreter according to the abstracting abstract machines approach
and observe the resulting implementation cannot analyze the example in
under 30 minutes.  In section~\ref{sec:baseline}, we perform a further
abstraction by widening the store.  The resulting analyzer sacrifices
precision for speed and is able to analyze the example in about 1
minute.  Without this abstraction step the analysis is exponential due
to per-state store variance.  This step is described by Van Horn and
Might~\cite[\S 3.5--6]{dvanhorn:VanHorn2012Systematic} and is
necessary to make even small examples feasible.  We therefore take the
widened interpreter as the baseline for our evaluation.

In section~\ref{sec:opt} we give a series of simple abstractions and
implementation techniques that, in total, drop the analysis time to
only 138 milliseconds, a speedup of 475 times.
Figure~\ref{fig:churchtime} shows the step-wise improvement of the
analysis time for this example.

\begin{figure}
\begin{center}
\includegraphics[width=2.5in]{rel-time-church.ps}
\end{center}
\caption{Relative run-time of
optimized analyzers to baseline for Church numeral example.}
\label{fig:churchtime}
\end{figure}

In section~\ref{sec:impl}, we describe a more complete implementation
scaled up and capable of analyzing Scheme benchmark programs.

In section~\ref{sec:eval} we evaluate this implementation against a
set of benchmark programs drawn from the literature.  Many benchmarks
cause the baseline analyzer to take longer than 30 minutes or to
consume more 1 gigabit of memory, at which point the analysis is
stopped.  This is the case for the largest benchmark program, which is
3,500 lines of code and takes under a minute in the most optimized
analyzer.  For those benchmarks that did complete on the baseline, the
optimized analyzer outperformed the baseline by a factor of
% 475
two to
% 4,382
three orders of magnitude.


\newcommand{\church}[1]{\(\ulcorner{\tt #1}\urcorner\)}

\begin{figure}
\begin{alltt}
(define (zero? n) ((n (\(\lambda\) (zx) #f)) #t))
(define (((mult m1) m2) mf) (m2 (m1 mf)))
(define ((((plus p1) p2) pf) x)
  ((p1 pf) ((p2 pf) x)))

(define (((pred n) rf) rx)
  (((n (\(\lambda\) (g) (\(\lambda\) (h) (h (g rf)))))
    (\(\lambda\) (ignored) rx))
   (\(\lambda\) (id) id)))

(define ((church=? e1) e2)
  (if (zero? e1) (zero? e2)
      (if (zero? e2) #f
          ((church=? (pred e1)) (pred e2)))))

;; multiplication distributes over addition
((church=? ((mult \church2) ((plus \church1) \church3)))
 ((plus ((mult \church2) \church1)) ((mult \church2) \church3)))
\end{alltt}
\caption{Church-numeral calculation}
\label{fig:church}
\end{figure}

Section~\ref{sec:related} relates this work to the literature and
section~\ref{sec:conclusion} concludes.



\section{An abstract$^{\text 2}$ machine for ISWIM}
\label{sec:aam}


ISWIM is a family of programming languages parameterized by a set of
base values and operations.  To make things concrete, we consider a
member of the ISWIM family with integers, booleans, and a few
operations.

\begin{figure}
\[
\begin{array}{l@{\qquad}rcl}
\text{Expressions} & \mathit{e} &=& \svar[^\mlab]\mvar\\
&&|& \slit[^\mlab]\mlit\\
&&|& \slam[^\mlab]\mvar\mexp\\
&&|& \sapp[^\mlab]\mexp\mexp \\
&&|& \sif[^\mlab]\mexp\mexp\mexp \\
\text{Variables}&\mvar &=& \syntax{x}\ |\ \syntax{y}\ |\ \dots\\
\text{Literals}&\mlit &=& \mnum\ |\ \mbln\ |\ \mop\\
\text{Integers}&\mnum &=& \syntax{0}\ |\ \syntax{1}\ |\ \syntax{-1}\ |\ \dots\\
\text{Booleans}&\mbln &=& \strue\ |\ \sfalse\\
\text{Operations}&\mop &=& \syntax{zero?}\ |\ \syntax{add1}\ |\ \syntax{sub1}\ |\ \dots
\end{array}
\]
\caption{Syntax of ISWIM}
\label{fig:syntax}
\end{figure}

Figure~\ref{fig:syntax} defines the (abstract) syntax of ISWIM.
Figure~\ref{fig:aam} defines the semantics of ISWIM as a machine
model.  Evaluation is defined as the set of states reachable by the
reflexive, transitive closure of the machine transition relation.  The
machine is a very slight variation on a standard abstract machine for
ISWIM in ``eval, continue, apply'' form.

Compared with the standard machine semantics, this definition is
different in the following ways:
\begin{itemize}
\item the store maps addresses to \emph{sets} of values,
\item continuations are heap-allocated,
\item there are ``contour values'' (written $\mcntr$) and syntax
  labels ($\mlab$) threaded through the computation, and
\item the machine is implicitly parameterized by the functions
  $\mathit{push}$, $\mathit{bind}$, $\interpdelta$ and $\sqcup$.
\end{itemize}

\begin{figure*}
\begin{align*}
\mathit{eval}(\mexp) &= \{ \mstate\ |\ \ev[^{\mtcntr}]{\mexp,\varnothing,\varnothing,\kmt} \multimachstep \mstate \} \text{ where }
\\[2mm]
%% EVAL
\ev{\svar\mvar,\menv,\msto,\mkont} &\machstep
\co{\mkont,\mval,\msto}
\text{ where }\mval \in \msto(\menv(\mvar))
\\
\ev{\slit\mlit,\menv,\msto,\mkont} &\machstep
\co{\mkont,\mlit,\msto}
\\
\ev{\slam\mvar\mexp,\menv,\msto,\mkont} &\machstep
\co{\mkont,\clos{\mvar,\mexp,\menv},\msto}
\\
\ev[^\mcntr]{\sapp[^\mlab]{\mexpi0}{\mexpi1},\menv,\msto,\mkont} &\machstep
\ev[^\mcntr]{\mexpi{0},\menv,\msto',\kar[_\mlab^\mcntr]{\mexpi{1},\menv,\maddr}}
\text{ where }\maddr,\msto' = \mathit{push}^\mcntr_\mlab(\msto,\mkont)
\\
\ev[^\mcntr]{\sif[^\mlab]{\mexpi0}{\mexpi1}{\mexpi2},\menv,\msto,\mkont} &\machstep
\ev[^\mcntr]{\mexpi0,\menv,\msto',\kif[^\mcntr]{\mexpi1,\mexpi2,\menv,\maddr}}
\text{ where }\maddr,\msto' = \mathit{push}_\mlab^\mcntr(\msto,\mkont)
\\[2mm]
%% CONTINUE
\co{\kmt,\mval,\msto} &\machstep
\ans{\msto,\mval}
\\
\co{\kar[^\mcntr_\mlab]{\mexp,\menv,\maddr},\mval,\msto} & \machstep
\ev[^\mcntr]{\mexp,\menv,\msto,\kfn[^\mcntr_\mlab]{\mval,\maddr}}
\\
\co{\kfn[^\mcntr_\mlab]{{\mvalx{u}},\maddr},\mval,\msto} & \machstep
\ap[^\mcntr_\mlab]{\mvalx{u},\mval,\mkont,\msto}
\text{ where }\mkont \in \msto(\maddr)
\\
\co{\kif[^\mcntr]{\mexpi0,\mexpi1,\menv,\maddr},\strue,\msto} & \machstep
\ev[^\mcntr]{\mexpi0,\menv,\msto,\mkont}
\text{ where }\mkont\in\msto(\maddr)
\\
\co{\kif[^\mcntr]{\mexpi0,\mexpi1,\menv,\maddr},\sfalse,\msto} & \machstep
\ev[^\mcntr]{\mexpi1,\menv,\msto,\mkont}
\text{ where }\mkont\in\msto(\maddr)
\\[2mm]
%% APPLY
\ap[^\mcntr_\mlab]{\clos{\mvar,\mexp,\menv},\mval,\msto,\mkont} & \machstep
\ev[^{\mcntr'}\!]{\mexp,\menv',\msto',\mkont}
\text{ where }\menv',\msto',\mcntr' = \mathit{bind\,}^{ \mcntr}_\mlab(\menv,\msto,\mvar,\mval)
\\
\ap[^\mcntr_\mlab]{\mop,\mval,\msto,\mkont} & \machstep
\co{\mkont,\mval',\msto}
\text{ where } \mval'\in\interpdelta(\mop,\mval)
\end{align*}
\caption{Abstract$^2$ machine for ISWIM}
\label{fig:aam}
\end{figure*}


\paragraph{Concrete interpretation} can be characterized by setting the implicit
parameters of the relation given in Figure~\ref{fig:aam} as follows:
\begin{align*}
\mathit{push}_\mlab^\mcntr(\msto,\mkont) &= \maddr,\msto\sqcup[\maddr\mapsto\{\mkont\}]
\mbox{ where }\maddr \notin\msto
\\
\mathit{bind}(\menv,\msto,\mvar,\mval) &= \menv[\mvar\mapsto\maddr],\msto\sqcup[\maddr\mapsto\{\mval\}]
\mbox{ where }\maddr \notin\msto
\end{align*}
The resulting relation is non-deterministic in its choice of
addresses, however it must always choose a fresh address when
allocating a continuation or variable binding.  If we consider machine
states equivalent up to consistent renaming, this relation defines
a deterministic machine.  (The relation is really a function.)


\paragraph{Abstract interpretation} can be characterized by setting the implicit
parameters just as above, but dropping the $\maddr \not\in \msto$
condition.  This family of interpreters is also non-deterministic in
choices of addresses, but it is free to choose addresses that are
already in use.  Consequently, the machines may be non-deterministic
when multiple values reside in a store location.

It is important to recognize from this definition that \emph{any}
allocation strategy is an abstract interpretation.  In particular,
concrete interpretation is a kind of abstract interpretation.  So is an
interpretation that allocates a single cell into which all bindings
and continuations are stored.  On the one hand is an abstract
interpretation that is non-computable and gives only the ground truth
of a programs behavior; on the other is an abstract interpretation
that is easy to compute but gives little information.  Useful program
analyses lay somewhere in between and can be characterized by their
choice of address representation and allocation strategy.

%% We now have a framework for describing program analysis for the ISWIM
%% family of languages, whereby approximation of both control and
%% environment structure is regulated by the heap and allocation
%% policies.

Uniform \(k\)-CFA is one such analysis.

\paragraph{Uniform \(k\)-CFA} can be characterized by the following allocation
strategy:

\begin{align*}
\mathit{push}_\mlab^\mcntr(\msto,\mkont) &=
  \mlab\mcntr,\msto\sqcup[\mlab\mcntr\mapsto\{\mkont\}] \\
\mathit{bind}^\mcntr_\mlab(\menv,\msto,\mvar,\mval) &= \menv[\mvar \mapsto \maddr],
                                           \msto\sqcup[\maddr \mapsto
                                             \{ \mval\}],
                                           \mcntr' \\
\mbox{where } \mcntr' &= \lfloor\mlab\mcntr\rfloor_k \\
              \maddr &= x\mcntr' \\
%%              \lfloor \mtcntr \rfloor_k &= \mtcntr \\
              \lfloor \mcntr \rfloor_0 &= \mtcntr \\
              \lfloor \mlab\mcntr \rfloor_{k+1} &= \mlab\lfloor \mcntr\rfloor_k
\end{align*}
For the duration of this paper, since our example abstract value space
has no interesting ordering, $\sqcup$ is a point-wise lifting of
$\cup$:
\begin{align*}
\msto\sqcup \varnothing &= \msto \\
\msto\sqcup \msto'[\maddr \mapsto S] &= \lambda \maddralt. \left\{
  \begin{array}{ll}
    \msto(\maddr)\cup S & \mbox{if } \maddralt = \maddr, \maddr\in dom(\msto) \\
    S & \mbox{if } \maddralt = \maddr, \maddr\notin dom(\msto) \\
    (\msto\sqcup\msto')(\maddralt) & \mbox{otherwise}
  \end{array}\right.
\end{align*}

\paragraph{Primitives}

\begin{align*}
\mnum+1 &\in \interpdelta(\saddone,\mnum) &
\mnum-1 &\in \interpdelta(\ssubone,\mnum)\\
\strue &\in \interpdelta(\szerohuh,\szero) &
\sfalse &\in \interpdelta(\szerohuh,\mnum)\text{ if }\mnum\neq \szero\\
\end{align*}

\begin{align*}
\sNum &\in \hat\interpdelta(\saddone,\mnum) &
\sNum &\in \hat\interpdelta(\ssubone,\mnum)\\
\strue &\in \hat\interpdelta(\szerohuh,\sNum) &
\sfalse &\in \hat\interpdelta(\szerohuh,\sNum)\\
\strue &\in \hat\interpdelta(\szerohuh,\szero) &
\sfalse &\in \hat\interpdelta(\szerohuh,\mnum)\text{ if }\mnum\neq \szero\\
\end{align*}

%% \begin{figure}
%% \begin{alltt}
%%   ;; State \(\to\) \(\Set(\)State\()\)
%%   (define (step \(\mstate\))
%%     (match \(\mstate\)
%%       [(ev \(\mexp\) \(\rho\) \(\msto\) \(\mkont\))
%%        (match \(\mexp\)
%%          [(var\(\superscript\mlab\) \(\mvar\))
%%           (for/set ((\(\mval\) (lookup \(\rho\) \(\msto\) \(\mvar\))))
%%             (co \(\msto\) \(\mkont\) \(\mval\)))]
%%          [(lit\(\superscript\mlab\) \(\mlit\)) (set (co \(\msto\) \(\mkont\) \(\mlit\)))]
%%          [(lam\(\superscript\mlab\) \(\mvar\) \(\mexp\)) (set (co \(\msto\) \(\mkont\) (clos \(\mvar\) \(\mexp\) \(\rho\))))]
%%          [(app\(\superscript\mlab\) \(\mexp\subscript0\) \(\mexp\subscript1\))
%%           (define-values (\(\msto'\) \(\maddr\)) (push \(\mstate\)))
%%           (set (ev \(\mexp\subscript0\) \(\rho\) \(\msto'\) (ar \(\mexp\subscript1\) \(\rho\) \(\maddr\))))]
%%          [(ife\(\superscript\mlab\) \(\mexp\subscript0\) \(\mexp\subscript1\) \(\mexp\subscript2\))
%%           (define-values (\(\msto'\) \(\maddr\)) (push \(\mstate\)))
%%           (set (ev \(\mexp\subscript0\) \(\rho\) \(\msto'\) (ifk \(\mexp\subscript1\) \(\mexp\subscript2\) \(\rho\) \(\maddr\))))])]
%%       [(co \(\msto\) \(\mkont\) \(\mval\))
%%        (match \(\mkont\)
%%          ['mt (set (ans \(\msto\) \(\mval\)))]
%%          [(ar\(\superscript\mlab\) \(\mexp\) \(\rho\)) (set (ev \(\mexp\) \(\rho\) \(\msto\) (fn \(\mval\) l)))]
%%          [(fn\(\superscript\mlab\) \(\mval'\))
%%           (for/set ((\(\mkont\) (get-cont \(\msto\) l)))
%%             (ap \(\msto\) \(\mval'\) \(\mval\) \(\mkont\)))]
%%          [(fi\(\superscript\mlab\) c a \(\rho\))
%%           (for/set ((k (get-cont \(\msto\) l)))
%%             (ev (if v c \(\maddr\)) \(\rho\) \(\msto\) \(\mkont\)))])]
%%       [(ap \(\msto\) fun \(\maddr\) \(\mkont\))
%%        (match fun
%%          [(clos l \(\mvar\) \(\mexp\) \(\rho\))
%%           (define-values (\(\rho'\) \(\msto'\)) (bind \(\mstate\)))
%%           (set (ev \(\msto'\) \(\mexp\) \(\rho'\) \(\mkont\)))]
%%          [(op \(\mop\))
%%           (for*/set ((\(\mkont\) (get-cont \(\msto\) l))
%%                      (\(\mval\) (\(\interpdelta\) \(\mop\) \(\mval\))))
%%             (co \(\msto\) \(\mkont\) \(\mval\)))]
%%          [_ (set)]))]))
%% \end{alltt}
%% \caption{Implementation of machine transition relation.}
%% \end{figure}

\newpage
\section{Reduction semantics to baseline analyzer}
\label{sec:baseline}

The uniform $k$-CFA allocation strategy would make the fixpoint of the
reduction relation in figure \ref{fig:aam} a computable analysis, but
not an effective one. This is not the strategy that AAM, nor we,
recommend. We explain a succession of approximations to reach the
baseline analysis that we compare performance against. But first, we
need a fixpoint combinator to drive the succession of reduction
relations we define.

\subsection{Generic fixpoint calculator}
\label{sec:fixpoint}

We compute the semantics of a program by iterating the state
transition relation until a fixed point in the reachable states is
reached. We begin with a generic fixpoint combinator, ${\mathcal F}$:

\begin{align*}
{\mathcal F}(f)(X) &= {\mathcal G}(f, X, \varnothing) \\
{\mathcal G}(f, \varnothing, S) &= S \\
{\mathcal G}(f, X, S) &= {\mathcal G}(f, f(X) \setminus S, S \cup X) \\
\end{align*}

The $\machstep$ relation is most naturally written as a function from
a state to a set of all next states. We want to find a fixpoint of
$\machstep$ on the state space, however, so we must lift it to work on
sets of states. The lifting is a natural one:

\begin{align*}
\lift(f)(X) &= \bigcup\{f(x) \mid x \in X\}
\end{align*}

Then $\eval(e) = {\mathcal F}(\lift(\machstep))(\{\ev[^\mtcntr]{e, \varnothing, \varnothing, \kmt}\})$.

If we run this on our Church numeral example, we must be
patient. Indeed, the state space is finite, but it is exponential in
the size of the program. With $k = 0$, the store can be in one of ${\mathcal
  O}(2^n)$ many configurations! We take a step toward a polynomial
complexity baseline by lifting the store out so that there is one
shared amongst all states.

\subsection{Store widening}
\label{sec:storewiden}

A common technique to faster convergence in flow analyses is to share
a common store that is monotonically updated as interpretation
continues. In the ISWIM language with only unary operators, this makes
our 0-CFA quartic in the size of the program. Not great, but
polynomial.

We can reuse the reduction relation we have defined, but with some
scaffolding around it. A state in the store-widened machine is a pair
of a set of ISWIM states without a store, and a store. A step of this
machine will pair the store with each of the states in this set,
reduce, and combine the output stores to create the next. The
scaffolded reduction relation, $\widehat{\machstep}$, is as follows:

Let $wn : \widehat{State} \times Store \to State$ be the metafunction
that adds a store to a storeless state and $c.\msto$ projects the
store out of a state $c$.

\begin{align*}
(cs, \msto) &\widehat{\machstep} (cs', \msto') \\
\mbox{where } cs' &= \{c' \mid wn(c, \msto) \machstep wn(c, \msto^c), c \in cs\} \\
              \msto' &= \bigsqcup\{\msto^c \mid wn(c,\msto)\machstep wn(c', \msto^c), c \in cs\}
\end{align*}
That is, the next set of states is what we get by stepping each
given state, without its store. The next store is the combination of all produced
stores. For simplicity, assume that stuck states step to themselves.

\subsection{Baseline evaluation}
\label{sec:baselineeval}

The final approximation we make to get to our baseline analysis is
store-allocating results of application sub-expressions. The semantics
of the previous section is where AAM leaves off. However, notice that
the $\kfn{}$ continuation stores a value - this makes the space of
continuations quadratic rather than linear in 0-CFA. A linear space
drops the complexity to cubic.

To achieve this, we instead allocate an address for the value position
when we create the continuation.  We will refer to this change in the
semantics as store-allocated results. This address and the tail address are
both determined by the label of the application point, so the space
becomes linear and the overall complexity drops to cubic. Note that
this is a critical abstraction in languages with n-ary functions,
since otherwise the continuation space becomes ${\mathcal O}(n^n)$!

The important rules that change are the following, specialized to 0-CFA:

\begin{align*}
\ev{\sapp[^\mlab]{\mexpi0}{\mexpi1},\menv,\msto,\mkont} &\machstep
\ev{\mexpi{0},\menv,\msto\sqcup[\mlab \mapsto \{\mkont\}],\kar{\mexpi{1},\mlab}}
\\
\co{\kar{\mexp,\menv,\mlab},\mval,\msto} & \machstep
\ev{\mexp,\menv,\msto\sqcup[\mlab^f \mapsto \{\mval\}],\kfn{\mlab^f,\mlab}}
\\
\co{\kfn{\mlab^f,\mlab},\mval,\msto} & \machstep
\ap[_\mlab]{\mvalx{u},\mlab^a,\mkont,\msto\sqcup[\mlab^a \mapsto \{\mval\}]}
\\
\text{ where } &\mkont \in \msto(\maddr), \mvalx{u} \in \msto(\maddr^f)
\\
\ap[_\mlab]{\clos{\mvar,\mexp,\menv},\mlab^a, \msto,\mkont} & \machstep
\ev{\mexp,\menv[\mvar\mapsto\mvar],\msto\sqcup[\mvar \mapsto \msto(\mlab^a)],\mkont}
\\
\ap[_\mlab]{\mop,\mlab^a,\msto,\mkont} & \machstep
\co{\mkont,\mval',\msto}
\\
\text{ where }& \mval'\in \{\interpdelta(\mop,\mval) \mid \mval \in \msto(\mlab^a)\}
\end{align*}

This extra store-allocation is effectively naming all intermediate
results, and thus the precision aligns with an analysis specialized to
ANF. We can also play a representation trick and remove
$\menv$ since we know it is the identity function in 0-CFA.

% Wide Store: cpu time: 551571 real time: 571319 gc time: 4003

\section{Implementation techniques}
\label{sec:opt}

We discuss a succession of techniques for increasing performance by
decreasing the interpretive overhead of abstract interpretation. They
come in two flavors: decreasing the size of the represented state
space [SS], and decreasing the amount of intermediate data structures [DS].

\begin{itemize}
 \item{[SS] quotient by sets of values (lazy non-determinism)}
 \item{[SS] remove {\tt ev} states (abstract compilation)}
 \item{[DS] operate on store changes rather than whole stores (store deltas)}
 \item{[DS] represent the store as a global, mutable hash and
   determine a state's store by the number of changes made to the
   global store at the time of visiting (timestamp)}
 \item{[DS] use a global mutable workset}
 \item{[DS] represent the global store as a global, mutable vector (preallocation)}
 \item{[SS] coarsely abstract literal data (abstracting compound data)}
\end{itemize}

Some techniques preserve the precision of the underlying analysis, and
others do not. We will discuss the design decisions behind the
precision-lossy techniques.  Each technique requires minimal changes
to the reduction relation/fixpoint, and each improves performance by
integer factors. The first three and last remain in the purely
functional realm. The final three use mutation in small, localized
places, so they do not increase conceptual complexity
drastically. Also, preallocation and mutable workset are standard
implementation practices \cite{ianjohnson:Shivers:1991:CFA}.

\subsection{Lazy non-determinism}

A shortcoming of the AAM approach is that much of the non-determinism
is spurious. For example, in a function application, {\tt (f x y)},
where all variables have been joined with multiple values, each
variable reference transitions to several new states that comprise the
cartesian product of all the values of {\tt f}, {\tt x} and {\tt y} (or in the our baseline analysis, the sum),
only to be joined back together in their respective application positions. We would
see transitions of the following shape:
\begin{center}
\includegraphics[scale=0.3]{fanout.pdf}
\end{center}
For this reason, we {\it delay} the non-determinism
until it is needed in {\it strict contexts} (such as the guard for an
{\tt if} a called procedure, or a numerical primitive application). We instead get transitions of this shape:
\begin{center}
\includegraphics[scale=0.3]{lazy.pdf}
\end{center}

For this, we introduce a new value for a delayed lookup that acts to
quotient the state space by the values at the delayed address. Its
changes to the semantics are highlighted in the following rules:

\begin{align*}
force(\msto,\saddr\maddralt) &= \msto(\maddralt)\\
force(\msto,\mval) &= \{\mval\}\\
\ev{\svar{\mvar},\menv,\mkont,\msto} &\machstep\;
\co{\mkont,\saddr{\menv(\mvar)},\msto} \\
\co{\kar[^\mcntr_\mlab]{\mexp,\menv,\maddr},\mval,\msto}
&\machstep\;
\ev[^\mcntr]{\mexp,\menv,\msto\sqcup[\maddr^f \mapsto force(\msto,\mval)],\kfn[^\mcntr_\mlab]{\maddr^f,\maddr}} \\
bind^\mcntr_\mlab(\menv,\msto,\mvar,\mval) &= \menv[\mvar \mapsto
  \maddr], \msto\sqcup[\maddr \mapsto force(\msto, \mval)], \mcntr' \\
\mbox{where } \mcntr' &= \lfloor\mlab\mcntr\rfloor_k \\
              \maddr &= \mvar\mcntr'
\end{align*}

\paragraph{Precision loss} This semantics introduces a subtle precision difference over the
baseline that is negligible. Consider a configuration where a
reference to a variable and a binding of a variable will happen in one
step. With laziness, the reference will mean the original binding(s)
of the variable or the new one, because the actual store lookup is
delayed precisely one step (i.e. laziness is administrative). Without
laziness, the reference will fan out to all the bindings of the
variable before the new binding happens and thus might have an
observable precision difference. In our experience, this situation
happens too rarely to warrant the significant increase in time and
memory needed for this eager non-determinism. Indeed, were the
variable reference a step later and another binding not made in
that step, the results of the two approaches are the same.

\paragraph{Avoiding precision loss} The administrative nature of laziness means that we could remove the
loss in precision by essentially duplicating the reduction relation to
specialize variable lookup. This works since in the semantics of ISWIM
with store-allocated results consumes the value component of states in
one step. This is not the case for semantics that replicate the value
component across reductions, say for popping off exception handler
frames. Further convolution is needed to remove the administrative
nature of laziness in these semantics. Due to the increase of
conceptual complexity for negligable benefit, we decided against this
approach.

% Lazy:  cpu time: 32481 real time: 32881 gc time: 547

\subsection{Abstract compilation}

Laziness removes several represented states that would then have to be
stepped by the fixpoint combinator.  We can further eliminate the {\tt
  ev} states from the run-time interpretation of a program by
specializing the machine transition relation to the program being
analyzed. This eliminates interpretative overhead by first compiling
the program into ``byte code'' instructions.

The essence of the compilation effect can be seen by considering an example
such as
\[
\sapp{\sapp{\sapp\mvar{\mexp_1}}{\mexp_2}}{\mexp_3}
\]
which makes the following transitions:
\begin{align}
& \ev{\sapp{\sapp{\sapp\mvar{\mexp_1}}{\mexp_2}}{\mexp_3},\menv,\mkont,\msto_0}\\
\machstep\; &
\ev{\sapp{\sapp\mvar{\mexp_1}}{\mexp_2},\menv,\kar{\mexp_3,\menv,\maddr_1},\msto_1}
\\
\machstep\; &
\ev{\sapp\mvar{\mexp_1},\menv,\kar{\mexp_2,\menv,\maddr_2},\msto_2}
\\
\machstep\; &
\ev{\mvar, \menv,\kar{\mexp_1,\menv,\maddr_3},\msto_3} % {\mexp_2}
\\
\machstep\; &
\co{\kar{\mexp_1,\menv},\mval,\msto_4} % {\mexp_1}{\mexp_2}
\mbox{ where } \mval \in \msto(\menv(\maddr))
\end{align}

where $\msto_4 = \msto_0 \sqcup \{ [\maddr_1 \mapsto \{ \mkont \}],
[\maddr_2 \mapsto \kar{\mexp_3,\menv,\maddr_1}]
[\maddr_3 \mapsto \kar{\mexp_2,\menv,\maddr_2}]$.

The compilation step converts expressions into functions that expect
the other components of the {\tt ev} state. Its definition in figure
\ref{fig:compile} shows close similarity to the rules for interpreting
    {\tt ev} states. The next step is to change creation of {\tt ev}
    states to simply calling these functions. The modified reduction
    relation is shown in figure \ref{fig:caam}.

\begin{figure}
\begin{align*}
\compile{\svar\mvar} &= \lambda(\menv,\msto,\mkont) .\co{\mkont,\mval,\msto} \text{ where }\mval\in\msto(\menv(\mvar))
\\
\compile{\slit\mlit} &= \lambda(\menv,\msto,\mkont) .
\co{\mkont,\mlit,\msto}
\\
\compile{\slam\mvar\mexp} &= \lambda(\menv,\msto,\mkont) .
\co{\mkont,\clos{\mvar,\compile\mexp,\menv},\msto}
\\
\compile{\sapp[^\mlab]{\mexpi0}{\mexpi1}} &= \lambda^\mcntr(\menv,\msto,\mkont) .
\compile{\mexpi0}^\mcntr(\menv,\msto',\kar[_\mlab^\mcntr]{\compile{\mexpi1},\menv,\maddr})
\\
&
\text{ where }\maddr,\msto' = \mathit{push}^\mcntr_\mlab(\msto,\mkont)
\\
\compile{\sif[^\mlab]{\mexpi0}{\mexpi1}{\mexpi2}} &= \lambda^\mcntr(\menv,\msto,\mkont) .
\compile{\mexpi0}^\delta(\menv,\msto',\kif[^\mcntr]{\compile{\mexpi1},\compile{\mexpi2},\menv,\maddr})
\\
&\text{ where }\maddr,\msto' = \mathit{push}_\mlab^\mcntr(\msto,\mkont)
\end{align*}
\caption{Compilation}
\label{fig:compile}
\end{figure}

\begin{figure}
\begin{align*}
\mathit{eval}(\mexp) &= \{ \mstate\ |\ \compile{\mexp}(\epsilon,\varnothing,\varnothing,\kmt) \multimachstep \mstate \} \text{ where }
\\[2mm]
%% CONTINUE
\co{\kmt,\mval,\msto} &\machstep
\ans{\msto,\mval}
\\
\co{\kar[^\mcntr_\mlab]{\mcomp,\menv,\maddr},\mval,\msto} & \machstep
\mcomp^\mcntr(\menv,\msto,\kfn[^\mcntr_\mlab]{\mval,\maddr})
\\
\co{\kfn[^\mcntr_\mlab]{{\mvalx{u}},\maddr},\mval,\msto} & \machstep
\ap[^\mcntr_\mlab]{\mval,\mvalx{u},\mkont,\msto}
\text{ where }\mkont \in \msto(\maddr)
\\
\co{\kif[^\mcntr]{\mcompi0,\mcompi1,\menv,\maddr},\strue,\msto} & \machstep
\mcompi0^\mcntr(\menv,\msto,\mkont)
\text{ where }\mkont\in\msto(\maddr)
\\
\co{\kif[^\mcntr]{\mcompi0,\mcompi1,\menv,\maddr},\sfalse,\msto} & \machstep
\mcompi1^\mcntr(\menv,\msto,\mkont)
\text{ where }\mkont\in\msto(\maddr)
\\[2mm]
%% APPLY
\ap[^\mcntr_\mlab]{\clos{\mvar,\mcomp,\menv},\mval,\msto,\mkont} & \machstep
\mcomp^{\mcntr'}(\menv',\msto',\mkont) \\
\text{ where }\menv',\msto',\mcntr' &= \mathit{bind\,}^{ \mcntr}_\mlab(\menv,\msto,\mvar,\mval)
\\
\ap{\mop,\mval,\msto,\mkont} & \machstep
\co{\mkont,\mval',\msto} \\
\text{ where }\mkont &\in\msto(\maddr)
\text{ and } \mval'\in\interpdelta(\mop,\mval)
\end{align*}
\caption{Abstract$^2$ machine for compiled ISWIM}
\label{fig:caam}
\end{figure}

% Compile: cpu time: 255397 real time: 261532 gc time: 2947

% \noindent
% Compile + Lazy: cpu time: 31173 real time: 31642 gc time: 739

%\newpage

\subsection{Store deltas}

Each step in the fixpoint for the above techniques requires joining
several, commonly large, stores together. Even with immutable hashes
that utilize sharing, this is memory- and time-intensive. Worse, this
contributes an obscene fraction of the runtime. If we instead track
the changes to the store made over all steps and afterward update the
store, we end up with much less allocation and computational overhead.

We represent changes as a list of pairs of address to set of storeable
values. Each $\msto\sqcup[\maddr \mapsto \mval{s}]$ now becomes
$\cons{\ttuple{\maddr}{\mval{s}}}{\msdiff}$, where $\msdiff$ begins as empty for
each step. Applying the changes is straightforward:

\begin{align*}
\mtlst(\msto) &= \msto \\
\cons{\ttuple{\maddr}{\mval{s}}}{\msdiff}(\msto) &= \msdiff(\msto)\sqcup[\maddr \mapsto \mval{s}]
\end{align*}

The reduction relation does have to change to accommodate this
technique, but only in this regard. The whole store is still given so
that lookups are possible. Example rule:

\begin{align*}
(\ap[^\mcntr_\mlab]{\clos{\mvar,\mexp,\menv},\mval,\mkont},\msto,\msdiff) & \machstep
(\ev[^{\mcntr'}]{\mexp,\menv',\mkont},\msdiff') \\
\text{ where }\menv',\msdiff',\mcntr' &= \mathit{bind\,}^{ \mcntr}_\mlab(\menv,\msto,\msdiff,\mvar,\mval)
\end{align*}
\begin{align*}
bind^{\mcntr}_\mlab(\menv, \msto, \msdiff, \mvar, \mval) &= \menv[\mvar \mapsto \maddr], \cons{\ttuple{\maddr}{force(\msto,\mval)}}{\msdiff}, \mcntr' \\
\text{ where } \mcntr' &= \lfloor \mlab \mcntr\rfloor_k \\
               \maddr &= \mvar\mcntr'
\end{align*}

We also lift $\machstep$ to accommodate for this asymmetry
in the input and output. For each state that is stepped, we feed the
output changes to the next so that all changes get accumulated.

\begin{align*}
(cs, \msto) &\widehat{\machstep} (cs\cup cs', \msdiff(\msto)) \\
\mbox{ where } (cs', \msdiff) &= step^*(\varnothing, cs, \mtlst) \\
step^*(S, \varnothing, \msdiff) &= (S, \msdiff) \\
step^*(S, \{c\}\cup cs, \msdiff &= step^*(S\cup cs^*, cs, \msdiff^*) \\
cs^* &= \{c' \mid (c,\msto,\msdiff) \machstep (c',\msdiff^c) \} \\
\msdiff^* &= concat\{\msdiff^c \mid (c, \msto,\msdiff) \machstep (c', \msdiff^c)\}
\end{align*}

%% Delta Store + Compile + Lazy:
%%    cpu time: 668 real time: 686 gc time: 41

\subsection{Timestamping an imperative store}

The store delta technique removes overhead of joining large stores,
but for determining if a state has been visited before (vis. $f(X)
\setminus S$ in ${\mathcal G}$), we still have to compare entire stores - an
expensive computation.  We modify a technique created in Shivers'
dissertation~\cite{ianjohnson:Shivers:1991:CFA}, timestamp
approximation, that removes the need for joining whole stores by
manipulating a global store imperatively. The original technique loses
precision because the starting store for each state in a machine
configuration can change between stepping all states and introduce
more flows than originally possible. However, by delaying the store
changes until after all steps have completed, we can maintain our
precision. In a variety of experiments, we observed that the overhead
for storing store-deltas was outweighed by the precision keeping flows
in check. That is to say, the analysis was faster {\it and} more
precise.

We pair this technique with a mutable workset. That is, to avoid the
intermediate sets introduced by the $State \to \Set(State)$ signature
of $\machstep$, we change the fixpoint algorithm to use a global
workset, {\tt todo}, that $\machstep$ imperatively updates with the
states that it reaches.

In total, we have six globals:
\begin{itemize}
 \item{{\tt $\msto$}, the store}
 \item{{\tt todo}, the workset}
 \item{{\tt seen}, a map from state to the timestamp at which it was last seen}
 \item{{\tt $\msdiff$}, the store changes for the current step}
 \item{{\tt $\msdiff$?}, a boolean tracking whether the stepped state contributed a store change}
 \item{{\tt T}, the timestamp of the store}
\end{itemize}

The new fixpoint calculation is defined in
figure~\ref{fig:imperative}.  To ensure termination, we guard adding
$c$ to {\tt todo} by the following check:
\begin{align*}\msdiff\mbox{{\tt ?}} \vee {\tt seen}(c) \neq {\tt T}\end{align*}
If it succeeds, {\tt todo} gets $c$ and we update {\tt seen} to map $c$ to {\tt T}.

After all steps complete, we apply $\msdiff$ to $\msto$ imperatively and increase
{\tt T} as long as there was at least one change in $\msdiff$. This
logic leads to termination if we know that each $\ttuple{\maddr}{\mval{s}}$
in $\msdiff$ would change the value of $\maddr$ in the current
store. Thus, we also guard additions to $\msdiff$ so that only updates
that would change the store are permitted. Each time $\msdiff$ is
successfully extended, we set {\tt $\msdiff$?} to true. Before each
individual step, we set it to false.

\begin{figure}
\begin{alltt}
eval(\(e\)) :=
  \(\msto\), todo, seen, T := \(\varnothing\), \(\varnothing\), [], 0
  \(\compile{e}\)(\(\mtcntr, \varnothing, \varnothing, \kmt\))
  while(true):
    if todo = \(\varnothing\): return (keys(seen), \(\msto\))
    else:
      let old := todo
      todo, \(\msdiff\) := \(\varnothing\), \(\mtlst\)
      foreach c \(\in\) old: \(\msdiff\)? := false; c()
      unless \(\msdiff\) = \(\mtlst\): T += 1; \(\msdiff(\msto)\)
\end{alltt}
\caption{Imperative fixpoint algorithm}
\label{fig:imperative}
\end{figure}

\subsection{Pre-allocating the store}
Hashes are not the most efficent for raw speed. If we commit to using
sequential natural numbers for variables and expression labels, by the
time we finish parsing the input program, we will know exactly how
many addresses we will need. Thus the store can be represented by a
vector the size of the largest label (call it $\mlab_{max}$) assigned, plus
1. The parser should be assigning new, fresh names to variables
anyway, so this change is a one-liner.

This technique by itself only works in 0-CFA, since there are many more
addresses that can be allocated in 1-CFA and higher. To support
general $k$-CFA in a vector representation, we use a growable vector
and a map from addresses to assigned indices. When assignable indices
run out, we grow the vector. The cost of this map is less than a hash
representation for the whole store since typically there are more
references to an address than allocations of it. This is more
space-efficient than preallocating $(\mlab_{max}+1)^k$ addresses
up-front and addressing by the contour converted to polynomial, since
many combinations of labels are not likely to occur. The all up-front
allocation again breaks down when we add compound data, which
introduce new kinds of labels (e.g. left and right components of a
tuple at the contour of creation).

\subsection{Abstracting Compound Data}

Practical issues of performance arise when one chooses the
``obvious,'' or ``natural'' abstraction for compound data. Our running
example will be the classic Boyer benchmark. The detail of note for
the Boyer benchmark is that it has large list literals, the exact
values of which do not particularly matter for many analyses. If we
interpret a list literal of 30 elements (that again have 10 elements
each) as 300 applications of {\tt cons}, and then interpret a function
that traverses the tree, we will end up interpreting that function at
least 300 times, when once or twice would do.

We will first explain the natural abstraction of tuples, and then we
will explain a less precise allocation strategy from uniform $k$-CFA
that we use for large compound data literals that we modified from the
implementation in \citep{dvanhorn:wright-jagannathan-toplas98}.

\subsubsection{Na\"ively}

The uniform way AAM approaches a simple abstraction strategy is to cut
recursion out of the data definition by tying the recursive knot
through the abstract store. For Scheme, the grammar for values looks
like the following:

\begin{alltt}
Value ::= #t | #f | (cons Value Value) | '() | ...
\end{alltt}

Upon evaluating a {\tt cons} application, we instead allocate two
addresses $a$ and $d$, join them to the respective values in the
store, and return the flattened {\tt (cons $a$ $d$)} value. Since
these addresses are all distinguished at different syntactic callsites
in the uniform $k$-CFA allocation strategy, and quoted lists are sugar
for a sequence of calls to {\tt cons}, this abstraction explodes the
value space. Analyzing a function that counts the number of atoms in a
literal s-expression would actually interpret that function at least
that number of times (more because of intermediate conses). Indeed,
even in our fastest implementation, this abstraction causes the
analysis of Boyer to be 430 times slower than the approach we will now
describe.

\subsubsection{Less na\"ively and less precisely}

The number of syntactic uses of {\tt cons} versus implicit uses via
literal lists is dramatically smaller in typical Scheme programs. We
use the above abstraction for these syntactic uses, but choose to
interpret literal lists as not always sugar for cascading {\tt
  cons}es. In particular, if a list literal is ``too big'' (in our
case length > 1), we interpret the list as a circular data structure;
the right address points back to the {\tt cons} itself, and the left
address points to all of the elements of the list. We take this
farther and join the list elements together in with a coarse
type-based abstraction. Quotation is special because it cannot
introduce function values, which is important to enhancing our
technique's soundness to conceptual complexity ratio. If we join two
values of different type together, we don't get ``anything,'' which
has complex meaning in higher-order languages (Shivers' escape
semantics) and is overly approximate. We instead get ``any quotable
value,'' which has much simpler semantics.

There are a few steps to consider:
\begin{itemize}
 \item{Define special value lattice elements for compound data domains that can be quoted
       (e.g. {\tt QPair} for $\left\{ ({\tt cons} a_i d_i)\right\}_i$, {\tt QVector} for immutable vectors, etc.)}
 \item{Define a ``larger'' value lattice element for all quotable data, {\tt QData}}
 \item{Interpret {\tt (quote (a ...))} as {\tt (qlist a ...)}, a new primitive function defined in figure \ref{fig:qlist},
       and similar definitions for immutable vectors.}
 \item{Extend the $\interpdelta$ axioms to include conservative
   meaning for these new values (e.g. {\tt (car QData)} = {\tt QData},
   {\tt (add1 QData)} = {\tt Number} and log ``possible type error'')
   and allow them to allocate addresses and change the store}
\end{itemize}

\begin{figure}
\begin{align*}
\interpdelta(\msto, qlist) &= (\mbox{{\tt '()}}, \msto) \\
\interpdelta(\msto, qlist, v ..._+) &= (({\tt cons}\ a\ d), \msto') \\
\qquad \mbox{where } \msto' &= \msto\sqcup[a \mapsto \bigsqcup(v ...)] \\
                            &\qquad \sqcup[d \mapsto \{ \mbox{{\tt '()}}, ({\tt cons}\ a\ d)\}] \\
\bigsqcup(v) &= v \\
\bigsqcup(v, vs ...) &= merge(v, \bigsqcup(vs ...)) \\
merge(v, v) &= v \\
merge(n, m) &= {\tt Number} \\
merge(n, v) &= {\tt QData} \\
merge(({\tt cons}\ a\ d), ({\tt cons}\ a'\ d')) &= {\tt QPair} \\
merge({\tt QPair}, ({\tt cons}\ a\ d)) &= {\tt QPair} \\
merge({\tt QPair}, v) &= {\tt QData} \\
\vdots
\end{align*}
\caption{Quoted list primitive}
\label{fig:qlist}
\end{figure}

\section{Evaluation}
\label{sec:eval}

\begin{figure*}
\begin{center}
\includegraphics[width=6in]{rel-time.ps}
\end{center}
\end{figure*}

\begin{figure}
\begin{center}
\includegraphics[width=3.2in]{abs-time.ps}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=3.2in]{state-per-sec.ps}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[width=3.2in]{peak-mem.ps}
\end{center}
\end{figure}


\cite{dvanhorn:Earl2012Introspective}

\cite{dvanhorn:wright-jagannathan-toplas98}

Other benchmarks

\section{Related work}
\label{sec:related}

Boucher and Feeley \cite{dvanhorn:Boucher1996Abstract} introduced the
idea of \emph{abstract compilation}, which used closure generation
\cite{dvanhorn:Feeley1987Using} to improve the performance of control
flow analysis.  We have adapted the closure generation technique from
composition evaluators to abstract machines and applied it to similar
effect.

\paragraph{Constraint-based program analysis for higher-order languages}

Constraint-based program analyses
(e.g.~\cite{dvanhorn:nielson-nielson-popl97,dvanhorn:wright-jagannathan-toplas98,dvanhorn:Meunier2006Modular})
typically compute sets of abstract values for each program point.
These values approximate the values arising at run-time for each
program point.  Value sets are computed as the least solution to a set
of constraints (either inclusion or equality constraints).  The
constraints must be designed and proved as a sound approximation of
the semantics.  Efficient implementations of these kinds of analyses
often take the form of worklist-based graph algorithms for constraint
solving, and are thus quite different from the interpreter
implementation.  The approach thus requires effort in constraint
system design and implementation, and the resulting system require
verification effort to prove the constraint system is sound and that
the implementation is correct.

This effort increases substantially as the complexity of the analyzed
language increases.  Both the work of maintaining the concrete
semantics and constraint system (and the relations between them) must
be scaled simultaneously.  However, constraint systems, which have
been extensively studied in their own right, enjoy efficient
implementation techniques and can be expressed in declarative logic
languages that are heavily
optimized~\cite{dvanhorn:bravenboer-smaragdakis-oopsla09}.
Consequently, constraint-based analyses can be computed quickly.  For
example, Jagannathan and Wright's polymorphic splitting
implementation~\cite{dvanhorn:wright-jagannathan-toplas98} analyses
the church numeral example of figure~\ref{fig:church} in 7
milliseconds, about 25 times faster than the fastest implementation
considered here.  These analyses compute very different things (as
detailed in section~\ref{sec:accept}), so the performance comparison
is not apples-to-apples.  We view the primary contribution of this
work as a systematic path that eases the design, verification, and
implementation of analyses using the abstracting abstract machine
approach to within a factor of performant constraint-based analyses.

\section{Conclusion}
\label{sec:conclusion}

Abstract machines are not only a good model for rapid analysis
development, they also can be systematically developed into efficient
algorithms.

%% \acks Sam Tobin-Hochstadt for encouragement and feedback -- he was
%% the first to prompt me to look into how make effective
%% implementations of the AAM approach.

%% NSF, DARPA

\paragraph{Acknowledgments}

We thank Suresh Jagannathan for providing source code to the
polymorphic splitting
analyzer~\cite{dvanhorn:wright-jagannathan-toplas98} and Ilya Sergey
for the introspective pushdown
analyzer~\cite{dvanhorn:Earl2012Introspective}.

\bibliographystyle{plain}
\bibliography{local,bibliography}

% \include{appendix}

\end{document}
