\documentclass{llncs}
\usepackage{alltt}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{stmaryrd}
\usepackage{xltxtra}
\setmonofont[Scale=MatchLowercase]{DejaVu Sans Mono}
%
\input{preamble}
\begin{document}
\title{Optimizing Abstract$^2$ Machines}
%
\frontmatter          % for the preliminaries
%
\author{J.~Ian Johnson\inst{1} \and Matthew Might\inst{2} \and David Van Horn\inst{1}}
\institute{Northeastern University \and University of Utah}

\maketitle              % typeset the title of the contribution

\begin{abstract}
The abstracting abstract machines approach to program analysis makes
it easy to design and implement sound and computable abstract
interpretations of higher-order programs.
%
However, the straightforward implementation is generally
not very fast.
%
This article contributes a step by step process for going from a naive
abstract$^2$ machine to an efficient program analyzer that is (nearly)
three orders of magnitude faster than the starting point, yet just as
precise (and purely functional).
\end{abstract}
%
\section{Introduction}

\subsection{Notation and prerequisites}

Prereqs: Semantics Engineering \cite{dvanhorn:Felleisen2009Semantics},
AAM \cite{dvanhorn:VanHorn2011Abstracting}
\cite{dvanhorn:VanHorn2012Systematic}.

Notation: concrete examples of programs under analysis is given in
(monochrome) Scheme notation.  Code is given in (syntax colored)
Racket.

\newpage
\section{Starting point}
\subsection{An abstract$^2$ machine for ISWIM}

ISWIM is a family of programming languages parameterized by a set of
base values and operations.  To make things concrete, we consider a
member of the ISWIM family with integers, booleans, and a few
operations.

\begin{figure}
\[
\begin{array}{l@{\qquad}rcl}
\text{Expressions} & \mathit{e} &=& \svar[^\mlab]\mvar\\
&&|& \slit[^\mlab]\mlit\\
&&|& \slam[^\mlab]\mvar\mexp\\
&&|& \sapp[^\mlab]\mexp\mexp \\
&&|& \sif[^\mlab]\mexp\mexp\mexp \\
\text{Variables}&\mvar &=& \syntax{x}\ |\ \syntax{y}\ |\ \dots\\
\text{Literals}&\mlit &=& \mnum\ |\ \mbln\ |\ \mop\\
\text{Integers}&\mnum &=& \syntax{0}\ |\ \syntax{1}\ |\ \syntax{-1}\ |\ \dots\\
\text{Booleans}&\mbln &=& \strue\ |\ \sfalse\\
\text{Operations}&\mop &=& \syntax{zero?}\ |\ \syntax{add1}\ |\ \syntax{sub1}\ |\ \dots
\end{array}
\]
\caption{Syntax of ISWIM}
\label{fig:syntax}
\end{figure}

Figure~\ref{fig:syntax} defines the (abstract) syntax of ISWIM.
Figure~\ref{fig:aam} defines the semantics of ISWIM as a machine
model.  Evaluation is defined as the set of states reachable by the
reflexive, transitive closure of the machine transition relation.  The
machine is a very slight variation on a standard abstract machine for
ISWIM in ``eval, continue, apply'' form.

Compared with the standard machine semantics, this definition is
different in the following ways:
\begin{itemize}
\item the store maps addresses to \emph{sets} of values,
\item continuations are heap-allocated,
\item there are ``contour values'' (written $\mcntr$) and syntax
  labels ($\mlab$) threaded through the computation, and
\item the machine is implicity parameterized by the functions
  $\mathit{push}$, $\mathit{bind}$, and $\interpdelta$.
\end{itemize}

\begin{figure}
\begin{align*}
\mathit{eval}(\mexp) &= \{ \mstate\ |\ \ev[^{\epsilon}]{\mexp,\varnothing,\varnothing,\kmt} \multimachstep \mstate \} \text{ where }
\\[2mm]
%% EVAL
\ev{\svar\mvar,\menv,\msto,\mkont} &\machstep
\co{\mkont,\mval,\msto}
\text{ where }\mval \in \msto(\menv(\mvar))
\\
\ev{\slit\mlit,\menv,\msto,\mkont} &\machstep
\co{\mkont,\mlit,\msto}
\\
\ev{\slam\mvar\mexp,\menv,\msto,\mkont} &\machstep
\co{\mkont,\clos{\mvar,\mexp,\menv},\msto}
\\
\ev[^\mcntr]{\sapp[^\mlab]{\mexpi0}{\mexpi1},\menv,\msto,\mkont} &\machstep
\ev[^\mcntr]{\mexpi{0},\menv,\msto',\kar[_\mlab^\mcntr]{\mexpi{1},\menv,\maddr}}
\text{ where }\maddr,\msto' = \mathit{push}^\mcntr_\mlab(\msto,\mkont)
\\
\ev[^\mcntr]{\sif[^\mlab]{\mexpi0}{\mexpi1}{\mexpi2},\menv,\msto,\mkont} &\machstep
\ev[^\mcntr]{\mexpi0,\menv,\msto',\kif[^\mcntr]{\mexpi1,\mexpi2,\menv,\maddr}}
\text{ where }\maddr,\msto' = \mathit{push}_\mlab^\mcntr(\msto,\mkont)
\\[2mm]
%% CONTINUE
\co{\kmt,\mval,\msto} &\machstep
\ans{\msto,\mval}
\\
\co{\kar[^\mcntr_\mlab]{\mexp,\menv,\maddr},\mval,\msto} & \machstep
\ev[^\mcntr]{\mexp,\menv,\msto,\kfn[^\mcntr_\mlab]{\mval,\maddr}}
\\
\co{\kfn[^\mcntr_\mlab]{{\mvalx{u}},\maddr},\mval,\msto} & \machstep
\ap[^\mcntr_\mlab]{\mval,\mvalx{u},\mkont,\msto}
\text{ where }\mkont \in \msto(\maddr)
\\
\co{\kif[^\mcntr]{\mexpi0,\mexpi1,\menv,\maddr},\strue,\msto} & \machstep
\ev[^\mcntr]{\mexpi0,\menv,\msto,\mkont}
\text{ where }\mkont\in\msto(\maddr)
\\
\co{\kif[^\mcntr]{\mexpi0,\mexpi1,\menv,\maddr},\sfalse,\msto} & \machstep
\ev[^\mcntr]{\mexpi1,\menv,\msto,\mkont}
\text{ where }\mkont\in\msto(\maddr)
\\[2mm]
%% APPLY
\ap[^\mcntr_\mlab]{\clos{\mvar,\mexp,\menv},\mval,\msto,\mkont} & \machstep
\ev[^{\mcntr'}\!]{\mexp,\menv',\msto',\mkont}
\text{ where }\menv',\msto',\mcntr' = \mathit{bind\,}^{ \mcntr}_\mlab(\msto,\mvar,\mval)
\\
\ap{\mop,\mval,\msto,\mkont} & \machstep
\co{\mkont,\mval',\msto}
\text{ where }\mkont\in\msto(\maddr)
\text{ and } \mval'\in\interpdelta(\mop,\mval)
\end{align*}
\caption{Abstract$^2$ machine for ISWIM}
\label{fig:aam}
\end{figure}


\paragraph{Concrete interpretation} can be characterized by setting the implicit
parameters of the relation given in Figure~\ref{fig:aam} as follows:
\begin{align*}
\mathit{push}(l,\msto,\mkont) &= \maddr,\msto\sqcup[\maddr\mapsto\{\mkont\}]
\mbox{ where }\maddr \notin\msto
\\
\mathit{bind}(\msto,\mvar,\mval) &= \menv[\mvar\mapsto\maddr],\msto\sqcup[\maddr\mapsto\{\mval\}]
\end{align*}
The resulting relation is non-deterministic in its choice of
addresses, however it must always choose a fresh address when
allocating a continuation or variable binding.  If we consider machine
states equivalent up to consistent renaming, this relation defines
a deterministic machine.  (The relation is really a function.)


\paragraph{Abstract interpretation} can be characterized by setting the implicit
parameters just as above, but dropping the $\maddr \not\in \msto$
condition.  This family of interpereters is also non-deterministic in
choices of addresses, but it is free to choose addresses that are
already in use.  Consequently, the machines may be non-deterministic
when mutiple values reside in a store location.

It is important to recognize from this definition that \emph{any}
allocation strategy is an abstract interpretation.  In particular,
concrete intepretation is a kind of abstract interpretation.  So is an
interpretation that allocates a single cell into which all bindings
and continuations are stored.  On the one hand is an abstract
intepretation that is non-computable and gives only the ground truth
of a programs behavior; on the other is an abstract interpretation
that is easy to compute but gives little information.  Useful program
analyses lay somewhere in between and can be characterized by their
choice of address representation and allocation strategy.

%% We now have a framework for describing program analysis for the ISWIM
%% family of languages, whereby approximation of both control and
%% environment structure is regulated by the heap and allocation
%% policies.

Uniform \(k\)-CFA is one such analysis.

\paragraph{Uniform \(k\)-CFA} can be characterized by the following allocation
strategy:

\begin{align*}
\mathit{push} &= \dots\\
\mathit{bind} &= \dots
\end{align*}

\paragraph{Primitives}



%% \begin{align*}
%% \widehat{\mathit{push}}(\maddr,\msto,\mkont) &= \maddr,\msto\sqcup[\maddr\mapsto\{\mkont\}]
%% % \mbox{ where }\maddr \notin\msto
%% \\
%% \widehat{\mathit{bind}}(\msto,\mvar,\mval) &= \menv[\mvar\mapsto\maddr],\msto\sqcup[\maddr\mapsto\{\mval\}]
%% \end{align*}


\begin{align*}
\mnum+1 &\in \delta(\saddone,\mnum) &
\mnum-1 &\in \delta(\ssubone,\mnum)\\
\strue &\in \delta(\szerohuh,\szero) &
\sfalse &\in \delta(\szerohuh,\mnum)\text{ if }\mnum\neq \szero\\
\end{align*}

\begin{align*}
\sNum &\in \hat\delta(\saddone,\mnum) &
\sNum &\in \hat\delta(\ssubone,\mnum)\\
\strue &\in \hat\delta(\szerohuh,\sNum) &
\sfalse &\in \hat\delta(\szerohuh,\sNum)\\
\strue &\in \hat\delta(\szerohuh,\szero) &
\sfalse &\in \hat\delta(\szerohuh,\mnum)\text{ if }\mnum\neq \szero\\
\end{align*}


\begin{figure}
\begin{alltt}
  ;; State \(\rightarrow\) Setof State
  (define (step state)
    (match state
      [(ev σ e ρ k)
       (match e
         [(var l x)           (for/set ((v (lookup ρ σ x))) (co σ k v))]
         [(num l n)           (set (co σ k n))]
         [(bln l b)           (set (co σ k b))]
         [(lam l x e)         (set (co σ k (clos l x e ρ)))]
         [(rec f (lam l x e)) (set (co σ k (rlos l f x e ρ)))]
         [(app l f e)
          (define-values (σ* a) (push state))
          (set (ev σ* f ρ (ar e ρ a)))]
         [(ife l e0 e1 e2)
          (define-values (σ* a) (push state))
          (set (ev σ* e0 ρ (ifk e1 e2 ρ a)))]
         [(1op l o e)
          (define-values (σ* a) (push state))
          (set (ev σ* e ρ (1opk o a)))])]
      [(co σ k v)
       (match k
         ['mt (set (ans σ v))]
         [(ar e ρ l) (set (ev σ e ρ (fn v l)))]
         [(fn f l)   (for/set ((k (get-cont σ l))) (ap σ f v k))]
         [(fi c a ρ l)
          (for/set ((k (get-cont σ l)))
            (ev σ (if v c a) ρ k))]
         [(oa o l)
          (for*/set ((k (get-cont σ l))
                     (v (delta o (list v))))
            (co σ k v))]
      [(ap σ fun a k)
       (match fun
         [(clos l x e ρ)
          (define-values (ρ* σ*) (bind state))
          (set (ev σ* e ρ* k))]
         [_ (set)])]
\end{alltt}
\caption{Implementation of machine transition relation.}
\end{figure}

\subsection{An example: Analyzing Church number computations}

\newcommand{\church}[1]{\(\ulcorner{\tt #1}\urcorner\)}

\begin{alltt}
;; multiplication distributes over addition
((church=? ((mult \church2) ((plus \church1) \church3)))
 ((plus ((mult \church2) \church1)) ((mult \church2) \church3)))))
\end{alltt}

Where \syntax{church=?} is an equality function for Church numerals
written in terms of recursion and an iterative \syntax{pred} function.
The \syntax{plus} and \syntax{mult} functions are written as usual.

This program reaches $N$ states under concrete interpretation.

\subsection{Generic fixpoint calculator}

We compute the semantics of a program by iterating the state
transition relation until a fixed point in the reachable states is
reached.

\begin{verbatim}
;; appl : (∀ (X) ((X -> (Setof X)) -> ((Setof X) -> (Setof X))))
(define ((appl f) s)
  (for/fold ([i (set)])
    ([x (in-set s)])
    (set-union i (f x))))

;; Calculate fixpoint of (appl f).
;; fix : (∀ (X) ((X -> (Setof X)) (Setof X) -> (Setof X)))
(define (fix f s)
  (let loop ((accum (set)) (front s))
    (if (set-empty? front)
        accum
        (let ((new-front ((appl f) front)))
          (loop (set-union accum front)
                (set-subtract new-front accum))))))
\end{verbatim}

\subsection{Store widening}

\begin{verbatim}
;; State^ = (cons (Set Conf) Store)

;; (State -> Setof State) -> State^ -> { State^ }
(define ((wide-step step) state)
  (match state
    [(cons cs σ)
     (define ss ((appl step)
                 (for/set ([c cs]) (c->s c σ))))
     (set (cons (for/set ([s ss]) (s->c s))
                (join-stores ss)))]))
\end{verbatim}

\subsection{Baseline evaluation}

Wide Store: cpu time: 551571 real time: 571319 gc time: 4003

\section{Precision preserving recipe}

\subsection{Lazy non-determinism}

Lazy:
   cpu time: 32481 real time: 32881 gc time: 547

\subsection{Abstract compilation}

We can eliminate the {\tt ev} states from the run-time interpretation
of a program by specializing the machine transition relation to the
program being analyzed.  This eliminates interpretative overhead by
first compiling the program into ``byte code'' instructions.

The essence of the compilation effect can be seen by consider an example
such as
\[
\sapp{\sapp{\sapp\mvar{\mexp_1}}{\mexp_2}}{\mexp_3}
\]
which makes the following transitions:
\begin{align}
& \ev{\sapp{\sapp{\sapp\mvar{\mexp_1}}{\mexp_2}}{\mexp_3},\menv,\mkont,\msto_0}\\
\machstep\; &
\ev{\sapp{\sapp\mvar{\mexp_1}}{\mexp_2},\menv,\kar{\mexp_3,\menv,\maddr_1},\msto_1}
\\
\machstep\; &
\ev{\sapp\mvar{\mexp_1},\menv,\kar{\mexp_2,\menv,\maddr_2},\msto_2}
\\
\machstep\; &
\ev{\mvar, \menv,\kar{\mexp_1,\menv,\maddr_3},\msto_3} % {\mexp_2}
\\
\machstep\; &
\co{\kar{\mexp_1,\menv},\mval,\msto_4} % {\mexp_1}{\mexp_2}
\mbox{ where } \mval \in \msto(\menv(\maddr))
\end{align}

where $\msto_4 = \msto_0 \sqcup \{ [\maddr_1 \mapsto \{ \mkont \}],
[\maddr_2 \mapsto \kar{\mexp_3,\menv,\maddr_1}]
[\maddr_3 \mapsto \kar{\mexp_2,\menv,\maddr_2}]$.


Notice that the only difference between concrete and abstract interpretation
is which addresses are pushed.

\begin{figure}
\begin{align*}
\compile{\svar\mvar} &= \lambda(\menv,\msto,\mkont) .\co{\mkont,\mval,\msto} \text{ where }\mval\in\msto(\menv(\mvar))
\\
\compile{\slit\mlit} &= \lambda(\menv,\msto,\mkont) .
\co{\mkont,\mlit,\msto}
\\
\compile{\slam\mvar\mexp} &= \lambda(\menv,\msto,\mkont) .
\co{\mkont,\clos{\mvar,\compile\mexp,\menv},\msto}
\\
\compile{\sapp[^\mlab]{\mexpi0}{\mexpi1}} &= \lambda^\mcntr(\menv,\msto,\mkont) .
\compile{\mexpi0}^\mcntr(\menv,\msto',\kar[_\mlab^\mcntr]{\compile{\mexpi1},\menv,\maddr})
\text{ where }\maddr,\msto' = \mathit{push}^\mcntr_\mlab(\msto,\mkont)
\\
\compile{\sif[^\mlab]{\mexpi0}{\mexpi1}{\mexpi2}} &= \lambda^\mcntr(\menv,\msto,\mkont) .
\compile{\mexpi0}^\delta(\menv,\msto',\kif[^\mcntr]{\compile{\mexpi1},\compile{\mexpi2},\menv,\maddr})
\text{ where }\maddr,\msto' = \mathit{push}_\mlab^\mcntr(\msto,\mkont)
\end{align*}
\caption{Compilation}
\end{figure}

\begin{figure}
\begin{align*}
\mathit{eval}(\mexp) &= \{ \mstate\ |\ \compile{\mexp}(\epsilon,\varnothing,\varnothing,\kmt) \multimachstep \mstate \} \text{ where }
\\[2mm]
%% CONTINUE
\co{\kmt,\mval,\msto} &\machstep
\ans{\msto,\mval}
\\
\co{\kar[^\mcntr_\mlab]{\mcomp,\menv,\maddr},\mval,\msto} & \machstep
\mcomp^\mcntr(\menv,\msto,\kfn[^\mcntr_\mlab]{\mval,\maddr})
\\
\co{\kfn[^\mcntr_\mlab]{{\mvalx{u}},\maddr},\mval,\msto} & \machstep
\ap[^\mcntr_\mlab]{\mval,\mvalx{u},\mkont,\msto}
\text{ where }\mkont \in \msto(\maddr)
\\
\co{\kif[^\mcntr]{\mcompi0,\mcompi1,\menv,\maddr},\strue,\msto} & \machstep
\mcompi0^\mcntr(\menv,\msto,\mkont)
\text{ where }\mkont\in\msto(\maddr)
\\
\co{\kif[^\mcntr]{\mcompi0,\mcompi1,\menv,\maddr},\sfalse,\msto} & \machstep
\mcompi1^\mcntr(\menv,\msto,\mkont)
\text{ where }\mkont\in\msto(\maddr)
\\[2mm]
%% APPLY
\ap[^\mcntr_\mlab]{\clos{\mvar,\mcomp,\menv},\mval,\msto,\mkont} & \machstep
\mcomp^{\mcntr'}(\menv',\msto',\mkont)
\text{ where }\menv',\msto',\mcntr' = \mathit{bind\,}^{ \mcntr}_\mlab(\msto,\mvar,\mval)
\\
\ap{\mop,\mval,\msto,\mkont} & \machstep
\co{\mkont,\mval',\msto}
\text{ where }\mkont\in\msto(\maddr)
\text{ and } \mval'\in\interpdelta(\mop,\mval)
\end{align*}
\caption{Abstract$^2$ machine for compiled ISWIM}
\label{fig:caam}
\end{figure}


Compile:
   cpu time: 255397 real time: 261532 gc time: 2947

\noindent
Compile + Lazy:
   cpu time: 31173 real time: 31642 gc time: 739

\subsection{Fixed-point specialization}

%% Should be commutted to beginning

Special + Compile + Lazy:
   cpu time: 14212 real time: 14681 gc time: 823

\subsection{Computing with store changes}

Delta Store + Special + Compile + Lazy:
   cpu time: 668 real time: 686 gc time: 41

\section{Evaluation}

\cite{dvanhorn:Earl2012Introspective}

\cite{dvanhorn:wright-jagannathan-toplas98}

Other benchmarks

\section{Related work}

Boucher and Feeley \cite{dvanhorn:Boucher1996Abstract} introduced the
idea of \emph{abstract compilation}, which used closure generation
\cite{dvanhorn:Feeley1987Using} to improve the performance of control
flow analysis.  We have adapted the closure generation technique from
composition evaluators to abstract machines and applied it to similar
effect.

\section{Conclusion}

Abstract machines are not only a good model for rapid analysis
development, they also can be systematically developed into efficient
algorithms.

\bibliographystyle{splncs}
\bibliography{dvh-bibliography}

\appendix
\section{Relation to Uniform \(k\)-CFA (A Case Against Acceptability)}

\cite{dvanhorn:nielson-nielson-popl97} \cite{dvanhorn:Neilson:1999}

This machine's allocation strategy mimics the Uniform k-CFA analysis
in Principles of Program Analysis, which is defined in terms of
``$\delta$ contours.''  However, because the machine represetation makes
context explicit via continuations, we can calculate these contours
rather than thread them throught the evaluator.  In other words, we
can use the CESK* machine without modification to obtain Uniform k-CFA
by way of a simple allocation strategy.  (In this way, it's a
simplification of the presentation in JFP.)

NNH uses a coinductive acceptability relation to specify Uniform
k-CFA:

\[
   C,R \models^{ce}_\delta E
\]

The cache and global environment form a finite store-like structure
holding bindings and return values.  The contour environment ce maps
variables to locations in R which contains their bindings, just as the
environment of the CESK* machine does.  The current contour delta is a
string of application labels describing the enclosing context under
which this term is being analyzed (or evaluated).  If you view the
acceptability relation as a big-step evalator, the
$(\widehat C,\widehat\rho)$ component should be seen as a global
store ce is the environment mapping variables to their locations.

Starting form the initial configuration for a program and iterating
the machine transition relation until reaching a fixpoint of reachable
states will \emph{underestimate} the acceptability relation of Uniform
k-CFA.  You can recover acceptability by feeding this store back into
the initial configuration and iterating again.  Repeating this process
until a complete run of the program reaches no new states will be the
least solution that is acceptable.

HOWEVER.  Why should we care about acceptability?  What this
machine computes is safe.  In other words, it computes a more
precise characterization of the run-time behavior of a program.  In
doing is so, it actually saves work (as can be seen above).

An Example:

\begin{verbatim}
 (let ((id (lambda (x) x)))
   (begin (id 1) (id 2)))
\end{verbatim}

Under Uniform 0-CFA, we would have:
\[
   [{\tt x} \mapsto \{{\tt 1}, {\tt 2}\}] \in \widehat\rho
\]

in the least solution to $\models$.  This says that, when run, 'x' is
bound to 1 or 2.

Under the machine semantics using a 0CFA allocation policy, the trace
semantics of the machine show that x is bound to 1, and that at some
later point, x becomes bound to 1 or 2.  Moreover, the machine would
show that (id 1) evaluates to 1 and only 1, while Uniform 0CFA must
give that (id 1) is either 1 or 2 to be acceptable.  We don't see any
value in these kinds of false flows that are due to the global and
timeless aspects of C,R which acceptability requires the heap to be
both finite and unchanging over the course of abstract
interpretation. (Another view of the difference: the machine abstracts
a program's execution as a \emph{finite state machine} that mimics the
machine interpretation of the program; the aceptability relation of
Uniform \(k\)-CFA abstracts a program's execution as a \emph{finite
  map} that mimics the big-step evaluator: from terms to (sets of)
values.)

\section{Pushdown Analysis}

It is straightforward to instantiate a \emph{pushdown} abstraction by
bounding only the variable binding portion of the heap, but using a
unique allocation strategy for continuations.  Such a strategy
abstracts a program's execution as a \emph{pushdown automata}
that mimics the machine interpretation.  This strategy therefore
models the abstract stack in a true stack like fashion and always
properly matches function calls with their return.

Although such analyses can be formulated straightforwardly in the
abstract machine approach, it is not clear all of the techniques of
this paper can be applied to similar effect in the pushdown context.
The main problem is calculation of an analysis can no longer be
computed as the fixed point of the machine transition relation.
Although there are several implementations (CFA2,ICFP'12), they
operate at speeds roughly on par with our starting point: unoptimized
store widened
machines. \cite{dvanhorn:Earl2012Introspective,dvanhorn:Vardoulakis2011CFA2}

\end{document}
